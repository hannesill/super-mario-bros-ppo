# -*- coding: utf-8 -*-
"""Copy of seminar_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14d0JZ8su4T8lrm0RG4hVgSjWCHuqOlUr

# Super Mario Bros Gym challenge
This is the example notebook for the seminar:

392241 Applied Cognitive Computing: Deep reinforcement learning with bounded rationality

# Setup
"""

"""
! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
! pip install gym-super-mario-bros opencv-python numpy tqdm
! pip install mediapy scipy
! pip install gym
"""

"""## Checking the gpu that is provided by google colab.
If you don't see a gpu listed, or an error is shown, please click on the small dropdown arrow on the top right corner and choose "View Ressources" from the appearing menu.

From there you can click on "Change runtime type" on the bottom right.

In the appearing window, you should be able to select a "Hardware accelerator", which you will have to set to "GPU"
"""

"""! nvidia-smi"""

"""# PPO Algorithm

Here we define the PPO Algorithm, that can be freely extended during the seminar.

The imports used for the PPO algorithm
"""

import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal
from torch.distributions import Categorical
from nes_py.wrappers import JoypadSpace
import gym_super_mario_bros
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
from datetime import datetime
import cv2
import numpy as np
from functools import partial

"""Setting the device to gpu if available"""

# set device to cpu or cuda
device = torch.device('cpu')
if torch.cuda.is_available():
    device = torch.device('cuda:0')
    torch.cuda.empty_cache()
    print("Device set to : " + str(torch.cuda.get_device_name(device)))

else:
    print("Device set to : cpu")
    raise ValueError("No GPU is set")

"""## ActorCritic
Here we define the actor critic agent that functions as our neural network.
To get a better understanding of the algorithm you can take a look at: https://theaisummer.com/Actor_critics/

## RolloutBuffer
"""

# The RolloutBuffer which keeps the training tuples.
class RolloutBuffer:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

    # We clear the buffer after each training update
    def clear(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):
        super(ActorCritic, self).__init__()
        # We define if the action space is continuous or a value choosen from a set of possible actions
        self.has_continuous_action_space = has_continuous_action_space

        if has_continuous_action_space:
            self.action_dim = action_dim
            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)

        # define the feature extractors (these are NOT shared between A/C contrary to stable baselines3)
        # given output shapes assume 4x downscaling
        self.actor = nn.Sequential(
            nn.Conv2d(state_dim[0], 16, kernel_size=7, stride=3, padding=(1, 0)),
            # output shape: (16, 19, 20)
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=(0, 1)),
            # output shape: (32, 8, 9)
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            # output shape: (64, 6, 7)
            nn.ReLU(),
            nn.Flatten(start_dim=-3, end_dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Conv2d(state_dim[0], 16, kernel_size=7, stride=3, padding=(1, 0)),
            # output shape: (16, 19, 20)
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=(0, 1)),
            # output shape: (32, 8, 9)
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),
            # output shape: (64, 6, 7)
            nn.ReLU(),
            nn.Flatten(start_dim=-3, end_dim=-1)
        )
        
        self.actor.apply(partial(self.init_weights, gain=np.sqrt(2)))
        self.critic.apply(partial(self.init_weights, gain=np.sqrt(2)))

        with torch.no_grad():
            sample = torch.rand(*state_dim)
            flatten_dim = self.actor(sample).shape[-1]

        # defining the actor
        if has_continuous_action_space:
            self.actor_list = nn.ModuleList([nn.Linear(flatten_dim, action_dim)])
        else:
            self.actor_list = nn.ModuleList([
                nn.Linear(flatten_dim, 64),
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, action_dim),
                nn.Softmax(dim=-1)
            ])
        self.actor_list.apply(partial(self.init_weights, gain=0.01))
        self.actor.extend(self.actor_list)

        # defining the critic
        self.critic_list = nn.ModuleList([
            nn.Linear(flatten_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        ])
        self.critic_list.apply(partial(self.init_weights, gain=1))
        self.critic.extend(self.critic_list)

    @staticmethod
    def init_weights(module: nn.Module, gain: float = 1) -> None:
        """
        Orthogonal weight initialization (cf. SB3: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py#L301)
        """
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            nn.init.orthogonal_(module.weight, gain=gain)
            if module.bias is not None:
                module.bias.data.fill_(0.0)

    def set_action_std(self, new_action_std):
        #We only calculate the action std if we have a continuous action space
        if self.has_continuous_action_space:
            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)
        else:
            print("--------------------------------------------------------------------------------------------")
            print("WARNING : Calling ActorCritic::set_action_std() on discrete action space policy")
            print("--------------------------------------------------------------------------------------------")

    def forward(self):
        raise NotImplementedError

    def act(self, state):
        # if we have a continuous action space we sample from a multivariate normal distribution
        # otherwise we calculate a categorical action spac
        if self.has_continuous_action_space:
            action_mean = self.actor(state)
            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)
            dist = MultivariateNormal(action_mean, cov_mat)
        else:
            action_probs = self.actor(state)
            dist = Categorical(action_probs)

        action = dist.sample()
        action_logprob = dist.log_prob(action)

        return action.detach(), action_logprob.detach()

    def evaluate(self, state, action):

        if self.has_continuous_action_space:
            action_mean = self.actor(state)

            action_var = self.action_var.expand_as(action_mean)
            cov_mat = torch.diag_embed(action_var).to(device)
            dist = MultivariateNormal(action_mean, cov_mat)

            # For Single Action Environments.
            if self.action_dim == 1:
                action = action.reshape(-1, self.action_dim)
        else:
            action_probs = self.actor(state)
            dist = Categorical(action_probs)
        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(state)

        return action_logprobs, state_values, dist_entropy

"""##PPO"""

class PPO:
    def __init__(self, state_dim, action_dim, lr_actor: float, lr_critic: float,
                 gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6,
                 lr_scheduler_args: dict = None, vf_coef: float = 0.5, ent_coef: float = 0.01):

        self.has_continuous_action_space = has_continuous_action_space

        if has_continuous_action_space:
            self.action_std = action_std_init

        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs
        self.vf_coef = vf_coef
        self.ent_coef = ent_coef

        self.buffer = RolloutBuffer()

        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)
        self.optimizer = torch.optim.Adam([
                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},
                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}
                    ])
        if lr_scheduler_args is not None:
            if len(lr_scheduler_args) == 3:
                self.scheduler = torch.optim.lr_scheduler.LinearLR(self.optimizer, **lr_scheduler_args)
            elif len(lr_scheduler_args) == 1:
                self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, **lr_scheduler_args)

        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())

        self.MseLoss = nn.MSELoss()

    def set_action_std(self, new_action_std):
        if self.has_continuous_action_space:
            self.action_std = new_action_std
            self.policy.set_action_std(new_action_std)
            self.policy_old.set_action_std(new_action_std)
        else:
            print("--------------------------------------------------------------------------------------------")
            print("WARNING : Calling PPO::set_action_std() on discrete action space policy")
            print("--------------------------------------------------------------------------------------------")

    def decay_action_std(self, action_std_decay_rate, min_action_std):
        print("--------------------------------------------------------------------------------------------")
        if self.has_continuous_action_space:
            self.action_std = self.action_std - action_std_decay_rate
            self.action_std = round(self.action_std, 4)
            if (self.action_std <= min_action_std):
                self.action_std = min_action_std
                print("setting actor output action_std to min_action_std : ", self.action_std)
            else:
                print("setting actor output action_std to : ", self.action_std)
            self.set_action_std(self.action_std)

        else:
            print("WARNING : Calling PPO::decay_action_std() on discrete action space policy")
        print("--------------------------------------------------------------------------------------------")

    def select_action(self, state):
        if self.has_continuous_action_space:
            with torch.no_grad():
                state = torch.FloatTensor(state).to(device)
                action, action_logprob = self.policy_old.act(state)

            self.buffer.states.append(state)
            self.buffer.actions.append(action)
            self.buffer.logprobs.append(action_logprob)

            return action.detach().cpu().numpy().flatten()
        else:
            with torch.no_grad():
                state = torch.FloatTensor(state).to(device)
                action, action_logprob = self.policy_old.act(state)

            self.buffer.states.append(state)
            self.buffer.actions.append(action)
            self.buffer.logprobs.append(action_logprob)

            return action.item()

    def update(self):
        # Monte Carlo estimate of returns
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)

        # Normalizing the rewards
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        # convert list to tensor
        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)
        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)
        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)

        # Optimize policy for K epochs
        for _ in range(self.K_epochs):

            # Evaluating old actions and values
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)

            # match state_values tensor dimensions with rewards tensor
            state_values = torch.squeeze(state_values)

            # Finding the ratio (pi_theta / pi_theta__old)
            ratios = torch.exp(logprobs - old_logprobs.detach())

            # Finding Surrogate Loss    
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages

            # final loss of clipped objective PPO
            loss = -torch.min(surr1, surr2) + self.vf_coef * self.MseLoss(state_values, rewards) - self.ent_coef * dist_entropy

            # take gradient step
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

        # Copy new weights into old policy
        self.policy_old.load_state_dict(self.policy.state_dict())
        
        # update learning rate
        if hasattr(self, "scheduler"):
            self.scheduler.step()

        # clear buffer
        self.buffer.clear()

    def save(self, checkpoint_path):
        torch.save(self.policy_old.state_dict(), checkpoint_path)

    def load(self, checkpoint_path):
        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))
        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))

"""##Helper Functions for the environments and rendering"""

# renders given frames with mediapy and shows a video
def renderEnv(run_id, epoch, frames):
    import mediapy as media
    media.show_video(frames, fps=60)
    media.write_video(f"videos_{run_id}/epoch_{epoch}.mp4", frames, fps=60)

# plot for visualizing results
def plotRewardandTime(run_id, avg_reward, avg_norm_reward, avg_length):
    import matplotlib.pyplot as plt
    from scipy.signal import savgol_filter
    plt.close()
    
    x = np.linspace(0,len(avg_reward),len(avg_reward))

    fig, axs = plt.subplots(2, 2,figsize=(9,9))
    axs[0, 0].plot(x, savgol_filter(avg_reward,window_length=29,polyorder=3,mode='mirror') if len(avg_reward) > 30 else avg_reward)
    axs[0, 0].set_title("avg_reward")

    axs[0, 1].plot(x, savgol_filter(avg_norm_reward,window_length=29,polyorder=3,mode='mirror') if len(avg_norm_reward) > 30 else avg_norm_reward)
    axs[0, 1].set_title("avg_norm_reward")

    axs[1, 0].plot(x, savgol_filter(avg_length,window_length=29,polyorder=3,mode='mirror') if len(avg_length) > 30 else avg_length)
    axs[1, 0].set_title("avg_length")
    plt.savefig(f"plots_{run_id}/{len(avg_length) - 1}.png")

import gym

# This environment wrapper is used to stop a run if mario is stuck on a pipe
class DeadlockEnv(gym.Wrapper):
    def __init__(self, env, threshold=10):
        super().__init__(env)
        self.last_world = 1
        self.last_stage = 1
        self.last_x_pos = 0
        self.count = 0
        self.threshold = threshold
        self.lifes = 3

    def reset(self, **kwargs):
        self.last_x_pos = 0
        self.count = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        state, reward, done, info = self.env.step(action)
        x_pos = info['x_pos']
        
        # if reached new stage
        if info["world"] > highest_world or (info["world"] == highest_world and info["stage"] > highest_stage):
            self.last_world = info["world"]
            self.last_stage = info["stage"]
            self.last_x_pos = 0

        if x_pos <= self.last_x_pos and info["world"] == highest_world and info["stage"] == self.last_stage:
            self.count += 1
        else:
            self.count = 0
            self.last_x_pos = x_pos

        if info['life'] != self.lifes:
            self.last_x_pos = x_pos
            self.count = 0
            self.lifes = info['life']

        if self.count >= self.threshold:
            reward = -15
            done = True

        return state, reward, done, info

# wrapper for bonus reward on flag_get
class FlagBonus(gym.Wrapper):
    def __init__(self, env, bonus=5):
        super().__init__(env)
        self.bonus = bonus

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        if info["flag_get"]:
            reward = np.clip(reward + self.bonus, -15, 15)

        return obs, reward, done, info

# wrapper to scale reward from (-15, 15) to (-1, 1)
class ScaleReward(gym.RewardWrapper):
    def __init__(self, env, ratio=15):
        super().__init__(env)
        self.ratio = ratio
        self.reward_range = (-15 / ratio, 15 / ratio)
    
    def reward(self, reward):
        return reward / self.ratio

# stack k frames
class FrameStack(gym.Wrapper):
    def __init__(self, env, k=4, obs_shape=None):
        super().__init__(env)
        self.k = k
        self.obs_shape = obs_shape
        self.frames = [np.zeros(obs_shape) for _ in range(k - 1)]
        
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.frames.append(obs)
        self.frames = self.frames[-self.k:]
        return self.frames, reward, done, info

# skipframe wrapper
class SkipFrame(gym.Wrapper):
    def __init__(self, env, skip):
        super().__init__(env)
        self._skip = skip

    def step(self, action):
        reward_out = 0
        for i in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            reward_out += reward
            if done:
                break
        reward_out /= max(1,i+1)

        return obs, reward, done, info

# downsample wrapper to reduce dimensionality
def Downsample(state, ratio: int) -> np.ndarray:
    (oldh, oldw, oldc) = state.shape
    newshape = (oldh//ratio, oldw//ratio, oldc)
    frame = cv2.resize(state, (newshape[1], newshape[0]), interpolation=cv2.INTER_AREA)
    return frame

# small function to change rgb images to grayscale
def GrayScale(state):
    return cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)

# SB3 image normalization
def NormalizeObservation(state):
    return state / 255

"""#Training"""

# from IPython.core.display import clear_output

if __name__ == "__main__":
    import argparse
    import wandb
    import os
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--max_training_epochs", type=int, default=int(1e7))
    parser.add_argument("--lr_actor", type=float, default=0.0003)
    parser.add_argument("--lr_critic", type=float, default=0.0003)
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--K_epochs", type=int, default=5)
    parser.add_argument("--down_sample_rate", type=int, default=4)
    parser.add_argument("--update_steps", type=int, default=2048)
    parser.add_argument("--vf_coef", type=float, default=0.5)
    parser.add_argument("--ent_coef", type=float, default=0.01)
    parser.add_argument("--use_lr_scheduler", action="store_true")
    parser.add_argument("--lr_scheduler_type", choices=["linear", "exponential"], default="linear")
    parser.add_argument("--lr_scheduler_start_factor", type=float, default=1.0)
    parser.add_argument("--lr_scheduler_end_factor", type=float, default=0.005)
    parser.add_argument("--lr_scheduler_total_iters", type=int, default=50000)
    parser.add_argument("--lr_gamma", type=float, default=500)
    args = parser.parse_args()
    
    print(args)
    
    with wandb.init(project="ACC", config=args, save_code=True) as run:
        os.mkdir(f"plots_{run.id}")
        os.mkdir(f"videos_{run.id}")
        
        image_space = (240, 256, 3)           # original image space
        state_dim = 15360                     # state space dimension
        lr_actor: float = args.lr_actor       # learning rate for actor network
        lr_critic: float = args.lr_critic     # learning rate for critic network (original: 0.001)
        gamma = args.gamma                    # gamma discount
        K_epochs = args.K_epochs              # K value for the PPO-CLIP objective function
        eps_clip = 0.2                        # the epsilon clipping value
        has_continuous_action_space = False   # the mario environment doesn't have a continuous action space
        action_std = None                     # we don't change the action distribution
        frameskip = 4                         # the frameskip value of the environment
        frame_stack = 4                       # frame stacking value
        down_sample_rate = args.down_sample_rate    # downsample rate. Calculated as: original_dimension/down_sample_rate
        modified_space = tuple([frame_stack, *[val // down_sample_rate for val in image_space[0:2]]])
        update_steps = args.update_steps      # how long we collect steps before updating
        vf_coef = args.vf_coef                # value function coefficient
        ent_coef = args.ent_coef              # entropy coefficient
        # the learning rate scheduler decays every update (not every episode or anything else)
        # default: half lr after 500 updates (~1mio timesteps or around reaching level 2)
        if args.use_lr_scheduler:
            # linear decay
            if args.lr_scheduler_type == "linear":
                lr_scheduler_args = {
                    "start_factor": args.lr_scheduler_start_factor,
                    "end_factor": args.lr_scheduler_end_factor,
                    "total_iters": args.lr_scheduler_total_iters
                }
            # exponential decay (half lr after <args.lr_gamma> updates)
            else:
                lr_scheduler_args = {
                    "gamma": 0.5 ** (1.0 / args.lr_gamma)
                }
        else:
            lr_scheduler_args = None

        # prepare env and wrappers
        env = gym_super_mario_bros.make('SuperMarioBros-v1')    # the environment. v0 is with original background, v1 has the background removed
        env = JoypadSpace(env, COMPLEX_MOVEMENT)                # The Joypadspace sets the available actions. We use COMPLEX_MOVEMENT to allow to go down in pipes.
        env = FlagBonus(env, bonus=5)                           # Flag bonus
        # env = ScaleReward(env, ratio=15)                        # Scale reward from (-15, 15) to (-1, 1)
        env = FrameStack(env, k=frame_stack, obs_shape=image_space)      # Frame stacking
        env = SkipFrame(env, skip=frameskip)                    # Skipframewrapper to skip some frames
        env = DeadlockEnv(env, threshold=10)                    # Deadlock environment wrapper to stop the game if mario is stuck at a pipe
        print(env.action_space)

        action_dim = env.action_space.n # action space dimension
        #state_dim = env.state_space.n  # Currently we flatten the input and therefore set the state_dim manually

        ppo_agent = PPO(modified_space, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std, lr_scheduler_args, vf_coef, ent_coef)

        # track total training time
        start_time = datetime.now().replace(microsecond=0)
        print("Started training at (GMT) : ", start_time)

        print("============================================================================================")

        # some helper variables
        time_step = 0
        max_training_epochs = args.max_training_epochs
        max_ep_len = 10000
        update_timestep = max_ep_len
        
        # furthest distance in highest level
        highest_x = -1
        
        # highest ever reached world/stage
        highest_world = 1
        highest_stage = 1

        avg_reward = []
        avg_length = []
        avg_norm_reward = []
        updates = 0
        
        info: bool | dict = None

        for i in range(max_training_epochs):
            # first we reset the state
            state = [*[np.zeros((modified_space[-2], modified_space[-1])) for _ in range(frame_stack - 1)], np.asarray(env.reset())]
            
            # only needed for the first run of every episode
            # prevents a bug, where cv2 falsely recognizes the first empty frames as depth images
            init_run = True
            
            current_ep_reward = 0

            # as we stack some frames, we create a buffer with empty frames for the first inputs
            frames = []

            # furthest distance in highest level of the current episode
            current_highest_x = -1
            
            # the collection loop
            for t in range(1, max_ep_len):
                # Downsampling the environment
                if init_run:
                    frames.append(state[-1].copy())
                    state[-1] = NormalizeObservation(GrayScale(Downsample(state[-1].copy(), ratio=down_sample_rate)))
                    init_run = False
                else:
                    for a in range(len(state)):
                        frames.append(state[a].copy())
                        state[a] = NormalizeObservation(GrayScale(Downsample(state[a].copy(), ratio=down_sample_rate)))

                # selecting an action
                action = ppo_agent.select_action(np.asarray(state))

                # performing the action and receiving the information from the environments
                state, reward, done, info = env.step(action)
                    
                # if reached new stage
                if info["world"] > highest_world or (info["world"] == highest_world and info["stage"] > highest_stage):
                    highest_world = info["world"]
                    highest_stage = info["stage"]
                    current_highest_x = -1
                    highest_x = -1
                
                # only update the furthest distance of the current episode if it is the highest ever reached level
                if info["x_pos"] > current_highest_x and info["world"] == highest_world and info["stage"] == highest_stage:
                    current_highest_x = info["x_pos"]

                # The PPO agent needs the reward and the done state manually, as we could modify it.
                ppo_agent.buffer.rewards.append(reward)
                ppo_agent.buffer.is_terminals.append(done)

                time_step +=1
                current_ep_reward += reward

                # every update_steps (2048) we update the algorithm
                if time_step % update_steps == 0:
                    ppo_agent.update()
                    updates += 1

                # if the run is done we break the loop
                if done:
                    break

            # We collect information every run and write them to the console
            avg_reward.append(current_ep_reward)
            avg_length.append(t)
            avg_norm_reward.append(current_ep_reward / max(1, t))
            
            # plot results if new record
            if current_highest_x > highest_x:
                highest_x = current_highest_x
                print("Epoch", i, "done:")
                print("Update iterations:", updates)
                print("Overall timesteps:", time_step)
                print("Statistics:")
                print("---")
                print("Reward for this episode:", current_ep_reward)
                print("Length of this episode:", t)
                print("---")
                print("Average total reward:", np.asarray(avg_reward[-50:]).mean())
                print("Average normalized reward:", np.asarray(avg_norm_reward[-50:]).mean())
                print("Average length:", np.asarray(avg_length[-50:]).mean())
                print("--------------------------------")
                plotRewardandTime(run.id, avg_reward, avg_norm_reward, avg_length)
                renderEnv(run.id, i, frames)
                wandb.log({"video": wandb.Video(f"videos_{run.id}/epoch_{i}.mp4", fps=60, format="mp4")}, commit=False)
                # alert new record only after 5000 episodes (to prevent spam)
                if i > 5000:
                    text = f"highest_world: {highest_world}\nhighest_stage: {highest_stage}\nhighest_x: {highest_x}\n{run.get_url()}"
                    wandb.alert(title=f"{run.name} achieved a new record", text=text)
                # We DON'T save the first epoch, as that would be just random weights.
                if i > 10:
                    ppo_agent.save(f"ppo_{run.id}.save")
            
            if args.use_lr_scheduler:
                lrs: list[float] = ppo_agent.scheduler.get_lr()
            else:
                lrs = [lr_actor, lr_critic]
            
            wandb.log({
                "timesteps": time_step,
                "updates": updates,
                "lr_actor": lrs[0],
                "lr_critic": lrs[1],
                "avg_reward": avg_reward[-1],
                "avg_norm_reward": avg_norm_reward[-1],
                "avg_length": avg_length[-1],
                "highest_x": highest_x,
                "current_highest_x": current_highest_x,
                "highest_total_stage": (highest_world - 1) * 8 + highest_stage,
                "current_total_stage": (info["world"] - 1) * 8 + info["stage"]
            })

        env.close()
